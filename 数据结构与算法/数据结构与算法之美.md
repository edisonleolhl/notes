# 数据结构与算法之美

- 数据结构和算法是相辅相成的。**数据结构是为算法服务的，算法要作用在特定的数据结构之上**

- 想要学习数据结构与算法，**首先要掌握一个数据结构与算法中最重要的概念——复杂度分析。**
- 王争总结的最常用、最基础的20个数据结构与算法
  - 10个数据结构：数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie树；
  - 10个算法：递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法。

## 复杂度分析

- 规律：**所有代码的执行时间T(n)与每行代码的执行次数f(n)成正比**。
- **大O时间复杂度表示法**：T(n)=O(f(n))
  - T(n)我们已经讲过了，它表示代码执行的时间；
  - n表示数据规模的大小；
  - f(n)表示每行代码执行的次数总和。
  - 公式中的O，表示代码的执行时间T(n)与f(n)表达式成正比。

- 大O时间复杂度实际上并不具体表示代码真正的执行时间，而是表示**代码执行时间随数据规模增长的变化趋势**，所以，也叫作**渐进时间复杂度**（asymptotic time complexity），简称**时间复杂度**。大O表示法通常会忽略掉公式中的常量、低阶、系数

- **加法法则：总复杂度等于量级最大的那段代码的复杂度**

- **乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积**

- **多项式量级**和**非多项式量级**。其中，非多项式量级只有两个：O(2n)和O(n!)。

  > 我们把时间复杂度为非多项式量级的算法问题叫作NP（Non-Deterministic Polynomial，非确定多项式）问题。

- 常见时间复杂度

  - O(1)：**一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)**

  - O(log2(n))：O(log3(n))也可以转换为O(log2(n))，因为log3(n)就等于log3(2) * log2(n)，log3(2)是常数系数，可以省略

    ```c++
    // 示例
    i=1;
     while (i <= n)  {
       i = i * 3;
     }
    ```

  - O(n*log2(n))：乘法法则，加一层n的嵌套，也很常见，比如归并排序、快速排序的时间复杂度都是O(nlogn)

  - **O(m+n)、O(m\*n)**：**由两个数据的规模**来决定，m和n是表示两个数据规模。我们无法事先评估m和n谁的量级大

- **最好情况时间复杂度**（best case time complexity）、**最坏情况时间复杂度**（worst case time complexity）
- **平均情况时间复杂度**（average case time complexity）：可能需要概率论的知识，但在大多数情况下，我们并不需要区分最好、最坏、平均情况时间复杂度三种情况，只有同一块代码在不同的情况下，时间复杂度有量级的差距，我们才会使用这三种复杂度表示法来区分。
- **均摊时间复杂度**（amortized time complexity）：比平均时间复杂度更特殊，**摊还分析法**：每一次O(n)的插入操作，都会跟着n-1次O(1)的插入操作，所以把耗时多的那次操作均摊到接下来的n-1次耗时少的操作上，均摊下来，这一组连续的操作的均摊时间复杂度就是O(1)

## 数组

- **数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据**

  > 除了数组，链表、队列、栈等也是线性表结构

- 高级语言一般提供了容器了代替数组，但数组在以下场景还是有用武之地的：
  - 1.Java ArrayList无法存储基本类型，比如int、long，需要封装为Integer、Long类，而Autoboxing、Unboxing则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。
  - 2.如果数据大小事先已知，并且对数据的操作非常简单，用不到ArrayList提供的大部分方法，也可以直接使用数组。
  - 3.还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如Object[][] array；而用容器的话则需要这样定义：`ArrayList<ArrayList > array

- 对于业务开发，直接使用容器就足够了，但是如果是底层开发，需要极致性能，数组会更优，当然debug难度也会更大

- 为什么数组要从0开始编号？

  - “下标”最确切的定义应该是“偏移（offset）”。如果用a来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址

    > ```c
    > // a[k]的内存地址计算公式
    > a[k]_address = base_address + k * type_size
    > // 二维数组的内存地址计算公式
    > a[x][y]_address = base_address + (y * x_array_length(X轴长度) + x) * data_type_size
    > 
    > ```

  - 后面出现的编程语言效仿C语言，也都用0作为下表了，但是Matlab是从1开始的，Python甚至还支持负数下标

## 链表

- LinkedHashMap就是使用的双向链表
- **用空间换时间**，双向链表虽然比单链表每个节点都多了一个前驱指针，但是查找与删除操作更加灵活
- 链表vs数组
  - 数组简单易用，在实现上使用的是连续的内存空间，可以借助CPU的缓存机制，预读数组中的数据，所以访问效率更高。而链表在内存中并不是连续存储，所以对CPU缓存不友好，没办法有效预读。
  - 数组的缺点是大小固定，一经声明就要占用整块连续内存空间，链表天然就支持扩容
  - 如果你的代码对内存的使用非常苛刻，那数组就更适合你，因为链表需要额外的空间存储指针

- 缓存淘汰算法，常见的策略有三种：先进先出策略FIFO（First In，First Out）、最少使用策略LFU（Least Frequently Used）、最近最少使用策略LRU（Least Recently Used）

- 有序单链表就可以实现LRU，当有一个新的数据被访问时，我们从链表头开始顺序遍历链表（因为这个操作，所以时间复杂度为O(n)）。

  1. 如果此数据之前已经被缓存在链表中了，我们遍历得到这个数据对应的结点，并将其从原来的位置删除，然后再插入到链表的头部。

  2. 如果此数据没有在缓存链表中，又可以分为两种情况：

     - 如果此时缓存未满，则将此结点直接插入到链表的头部；

     - 如果此时缓存已满，则链表尾结点删除，将新的数据结点插入链表的头部。

  3. 引入**散列表**（Hash table）来记录每个数据的位置，将缓存访问的时间复杂度降到O(1)

- 如果字符串是通过单链表来存储的，那该如何来判断是一个回文串呢？

  1、通过快慢指针定位中间节点

  2、对后半部分链表逆序

  3、前后链表对比，判断是否为回文串

  4、恢复后半部分链表顺序

  时间复杂度O(n)、空间复杂度O(1)

- 写链表代码的技巧

  - 理解指针或引用的含义，p->next=q，p结点中的next指针存储了q结点的内存地址。

  - 警惕指针丢失和内存泄漏，**插入结点时，一定要注意操作的顺序**，**删除链表结点时，也一定要记得手动释放内存空间**，

  - 利用哨兵简化实现难度，**针对链表的插入、删除操作，需要对插入第一个结点和删除最后一个结点的情况进行特殊处理**，在任何时候，不管链表是不是空，head指针都会一直指向这个哨兵结点。我们也把这种有哨兵结点的链表叫**带头链表**。相反，没有哨兵结点的链表就叫作**不带头链表**。

  - 重点留意边界条件处理

    - 如果链表为空时，代码是否能正常工作？

    - 如果链表只包含一个结点时，代码是否能正常工作？

    - 如果链表只包含两个结点时，代码是否能正常工作？

    - 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？

  - 举例画图，辅助思考

  - 多写多练，没有捷径

    - 单链表反转

    - 链表中环的检测

    - 两个有序的链表合并

    - 删除链表倒数第n个结点

    - 求链表的中间结点

## 栈

- 栈只支持两个基本操作：**入栈push()和出栈pop()**
- 从功能上来说，数组或链表确实可以替代栈，但是数组或链表暴露了太多操作接口，操作比较不可控，容易出错
- 复习一下均摊分析法，动态扩容的数组/栈，k-1次插入操作都是O(1)，然后来一次顺序搬迁需要O(n)，但是均摊到前面k-1次O(1)的操作上，所以摊还分析时间复杂度仍然是O(1)

- 应用

  - 函数栈

  - 表达式求值，一个栈保存操作数，一个栈保存运算符

    - 如果比运算符栈顶元素的优先级高，就将当前运算符压入栈；
    - 如果比运算符栈顶元素的优先级低或者相同，从运算符栈中取栈顶运算符，从操作数栈的栈顶取2个操作数，然后进行计算，再把计算完的结果压入操作数栈，继续比较。

    ![image-20220117002103552](https://tva1.sinaimg.cn/large/008i3skNgy1gyfz2tvz1oj315e0s4wgv.jpg)

  - 括号匹配：我们用栈来保存未匹配的左括号
    - 从左到右依次扫描字符串。当扫描到左括号时，则将其压入栈中；当扫描到右括号时，从栈顶取出一个左括号。
    - 如果扫描的过程中，遇到不能配对的右括号，或者栈中没有数据，则说明为非法格式。
    - 当所有的括号都扫描完成之后，如果栈为空，则说明字符串为合法格式；否则，说明有未匹配的左括号，为非法格式。
  - 浏览器的前进与后退：我们使用两个栈，X和Y，
    - 我们把首次浏览的页面依次压入栈X，
    - 当点击后退按钮时，再依次从栈X中出栈，并将出栈的数据依次放入栈Y。
    - 当我们点击前进按钮时，我们依次从栈Y中取出数据，放入栈X中。
    - 当栈X中没有数据时，那就说明没有页面可以继续后退浏览了。当栈Y中没有数据，那就说明没有页面可以点击前进按钮浏览了。

## 队列

- 最基本的操作也是两个：**入队enqueue()**，放一个数据到队列尾部；**出队dequeue()**，从队列头部取一个元素。
- 可以用数组，也可以用链表实现丢咧，用数组实现的叫顺序队列，用链表实现的叫链式队列
- 应用广泛，特别是一些具有某些额外特性的队列，比如循环队列、阻塞队列、并发队列。
- 它们在很多偏底层系统、框架、中间件的开发中，起着关键性的作用。比如高性能队列Disruptor、Linux环形缓存，都用到了循环并发队列；Java concurrent并发包利用ArrayBlockingQueue来实现公平锁等

- 循环队列，实现起来最关键是**确定好队空和队满的判定条件**。
  - 队列为空的判断条件仍然是head == tail
  - 队列为满的判断条件是**(tail+1)%n=head**，tail指向的位置没有存放数据，所以会浪费一个元素的数组空间

- **阻塞队列**其实就是在队列基础上增加了阻塞操作，阻塞队列可以轻松实现生产者-消费者模型。

  - 简单来说，就是在队列为空的时候，从队头取数据会被阻塞。因为此时还没有数据可取，直到队列中有了数据才能返回；

  - 如果队列已经满了，那么插入数据的操作就会被阻塞，直到队列中有空闲位置后再插入数据，然后再返回。

- **并发队列**是线程安全的队列。
  - 最简单直接的实现方式是直接在enqueue()、dequeue()方法上加锁，但是锁粒度大并发度会比较低，同一时刻仅允许一个存或者取操作。
  - 实际上，基于数组的循环队列，利用CAS原子操作，可以实现非常高效的并发队列。这也是循环队列比链式队列应用更加广泛的原因

- 队列可以解决在线程池等有限资源场景的问题
  - 非阻塞方式可以在队列为空时直接拒绝请求，阻塞队列可以将请求排队，等资源空闲时再取出来回应请求
  - 基于链表的实现方式，可以实现一个支持无限排队的无界队列（unbounded queue），但是可能会导致过多的请求排队等待，请求处理的响应时间过长。
  - 而基于数组实现的有界队列（bounded queue），队列的大小有限，所以线程池中排队的请求超过队列大小时，接下来的请求就会被拒绝，这种方式对响应时间敏感的系统来说，就相对更加合理。

## 递归

- 三个条件

  **1.一个问题的解可以分解为几个子问题的解**

  **2.这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样**

  **3.存在递归终止条件**

- 很多时候，递归理解的难点在于人脑习惯平铺直叙，习惯把递归展开，实际上，我们考虑问题A可以分为子问题B、C、D后，可以直接认为B、C、D已经解决了，只需考虑A与B、C、D的关系即可
- **编写递归代码的关键是，只要遇到递归，我们就把它抽象成一个递推公式，不用想一层层的调用关系，不要试图用人脑去分解递归的每个步骤**。

- 递归代码要注意堆栈溢出，因为函数栈是需要内存空间的
- 递归代码要警惕重复计算，比如可以用散列表存储已经计算过的函数值
- 递归需要额外的空间，所以空间复杂度不能忽略

## 排序

- 执行效率
  - 排序算法要考虑最好、最坏、平均时间复杂度，因为待排序数据可能是接近无序或接近有序，
  - 时间复杂度反映的是数据规模n很大的时候的一个增长趋势，所以它表示的时候会忽略系数、常数、低阶。但是实际的软件开发中，我们排序的可能是10个、100个、1000个这样规模很小的数据，所以，在对同一阶时间复杂度的排序算法性能对比的时候，我们就要把系数、常数、低阶也考虑进来。
  - 基于比较的排序算法的执行过程，会涉及两种操作，一种是元素比较大小，另一种是元素交换或移动。所以，如果我们在分析排序算法的执行效率的时候，应该把比较次数和交换（或移动）次数也考虑进去。

- 内存消耗
  - 原地排序算法，就是特指空间复杂度是O(1)的排序算法
- 是否稳定
  - 稳定性：如果待排序的序列中存在值相等的元素，经过排序之后，相等元素之间原有的先后顺序不变。
  - **稳定排序算法可以保持金额相同的两个对象，在排序之后的前后顺序不变**
  - 实际开发中，我们可能是对键值对中的键进行排序，值是否保序也是我们关注的点

### 冒泡排序（Bubble Sort）

- 冒泡排序只会操作相邻的两个数据。每次冒泡操作都会对相邻的两个元素进行比较，看是否满足大小关系要求。如果不满足就让它俩互换。一次冒泡会让至少一个元素移动到它应该在的位置，重复n次，就完成了n个数据的排序工作。

- 优化：当某次冒泡操作已经没有数据交换时，说明已经达到完全有序，不用再继续执行后续的冒泡操作

  ```java
  // 冒泡排序，a表示数组，n表示数组大小
  public void bubbleSort(int[] a, int n) {
    if (n <= 1) return;
   
   for (int i = 0; i < n; ++i) {
      // 提前退出冒泡循环的标志位
      boolean flag = false;
      for (int j = 0; j < n - i - 1; ++j) {
        if (a[j] > a[j+1]) { // 交换
          int tmp = a[j];
          a[j] = a[j+1];
          a[j+1] = tmp;
          flag = true;  // 表示有数据交换      
        }
      }
      if (!flag) break;  // 没有数据交换，提前退出
    }
  }
  ```

- 原地排序

- 稳定（当前后两个元素大小一样时不做交换即可实现稳定）

- 最好时间复杂度：O(1)（有序时，只需要一次冒泡即可完成）

- 最坏时间复杂度：O(n^2)（完全倒序时，需要n次冒泡）

- **有序度**是数组中具有有序关系的元素对的个数，完全有序的数组的有序度叫作**满有序度**，**逆序度=满有序度-有序度**

  - 有序元素对：a[i] <= a[j], 如果i < j
  - 对于一个倒序排列的数组，比如6，5，4，3，2，1，有序度是0；对于一个完全有序的数组，比如1，2，3，4，5，6，有序度就是**n\*(n-1)/2**，也就是15

- 平均时间复杂度：O(n^2)

### 插入排序（Insertion Sort）

- 首先，我们将数组中的数据分为两个区间，**已排序区间**和**未排序区间**。初始已排序区间只有一个元素，就是数组的第一个元素。插入算法的核心思想是取未排序区间中的元素，在已排序区间中找到合适的插入位置将其插入，并保证已排序区间数据一直有序。重复这个过程，直到未排序区间中元素为空，算法结束。

  ![image-20220119000023901](https://tva1.sinaimg.cn/large/008i3skNly1gyi9pwfvv9j30s60pujti.jpg)

- 插入排序也包含两种操作，一种是**元素的比较**，一种是**元素的移动**。当我们需要将一个数据a插入到已排序区间时，需要拿a与已排序区间的元素依次比较大小，找到合适的插入位置。找到插入点之后，我们还需要将插入点之后的元素顺序往后移动一位，这样才能腾出位置给元素a插入。

- 移动操作的次数总是固定的，就等于逆序度

  ```java
  // 插入排序，a表示数组，n表示数组大小
  public void insertionSort(int[] a, int n) {
    if (n <= 1) return;
  
    for (int i = 1; i < n; ++i) {
      int value = a[i];
      int j = i - 1;
      // 查找插入的位置
      for (; j >= 0; --j) {
        if (a[j] > value) {
          a[j+1] = a[j];  // 数据移动
        } else {
          break;
        }
      }
      a[j+1] = value; // 插入数据
    }
  }
  
  ```

- 原地排序

- 稳定（对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，即可实现稳定

- 最好时间复杂度：O(n)，**从尾到头遍历已经有序的数据**时才能实现

- 最坏时间复杂度：O(n^2)，如果数组是倒序的，每次插入都相当于在数组的第一个位置插入新的数据

- 平均时间复杂度：O(n^2)，在数组中插入一个数据的平均时间复杂度是O(n)，要插入n次，所以是O(n^2)

- 插入排序与冒泡排序的时间复杂度一样，元素交换的次数都等于原始数据的逆序度，**但是插入排序只需要一次数据移动操作，冒泡操作**（见代码）

### 选择排序（Selection Sort）

- 选择排序算法的实现思路有点类似插入排序，也分已排序区间和未排序区间。但是选择排序每次会从未排序区间中找到最小的元素，将其放到已排序区间的末尾。

- 已排序区间的末尾是待排序的第一个，所以会交换

  ![image-20220118235546819](https://tva1.sinaimg.cn/large/008i3skNly1gyi9l56mg0j31330u0n0c.jpg)

- 原地排序
- 最好、最坏、平均时间复杂度都是O(n^2)
- **不稳定**，因为会交换，破坏了稳定性

### 归并排序（MergeSort）

- 如果要排序一个数组，我们先把数组从中间分成前后两部分，然后对前后两部分分别排序，再将排好序的两部分合并在一起，这样整个数组就都有序了

- **分治思想**，一般用递归实现

  ```
  // 归并排序算法, A是数组，n表示数组大小
  merge_sort(A, n) {
    merge_sort_c(A, 0, n-1)
  }
  
  // 递归调用函数
  merge_sort_c(A, p, r) {
    // 递归终止条件
    if p >= r  then return
  
    // 取p到r之间的中间位置q
    q = (p+r) / 2
    // 分治递归
    merge_sort_c(A, p, q)
    merge_sort_c(A, q+1, r)
    // 将A[p...q]和A[q+1...r]合并为A[p...r]
    merge(A[p...r], A[p...q], A[q+1...r])
  }
  ```

- merge函数的过程如图所示

  ![image-20220119000510869](https://tva1.sinaimg.cn/large/008i3skNly1gyi9uvftmwj31380u0tcj.jpg)

  - 我们申请一个临时数组tmp，大小与A[p...r]相同。我们用两个游标i和j，分别指向A[p...q]和A[q+1...r]的第一个元素。比较这两个元素A[i]和A[j]，如果A[i]<=A[j]，我们就把A[i]放入到临时数组tmp，并且i后移一位，否则将A[j]放入到数组tmp，j后移一位。

  - 继续上述比较过程，直到其中一个子数组中的所有数据都放入临时数组中，再把另一个数组中的数据依次加入到临时数组的末尾，这个时候，临时数组中存储的就是两个子数组合并之后的结果了。最后再把临时数组tmp中的数据拷贝到原数组A[p...r]中。

    ```
    merge(A[p...r], A[p...q], A[q+1...r]) {
      var i := p，j := q+1，k := 0 // 初始化变量i, j, k
      var tmp := new array[0...r-p] // 申请一个大小跟A[p...r]一样的临时数组
      while i<=q AND j<=r do {
        if A[i] <= A[j] {
          tmp[k++] = A[i++] // i++等于i:=i+1
        } else {
          tmp[k++] = A[j++]
        }
      }
      
      // 判断哪个子数组中有剩余的数据
      var start := i，end := q
      if j<=r then start := j, end:=r
      
      // 将剩余的数据拷贝到临时数组tmp
      while start <= end do {
        tmp[k++] = A[start++]
      }
      
      // 将tmp中的数组拷贝回A[p...r]
      for i:=0 to r-p do {
        A[p+i] = tmp[i]
      }
    }
    ```

- 稳定：如果A[p...q]和A[q+1...r]之间有值相同的元素，那我们可以像伪代码中那样，先把A[p...q]中的元素放入tmp数组。这样就保证了值相同的元素，在合并前后的先后顺序不变

- 不是原地排序，需要O(n)的额外空间存放临时数组（所以应用不如快排广泛

- 递归的时间复杂度分析

  - 假设问题a可以分解为b与c，b与c解决后，再把b、c的结果合并成a的结果，则T(a) = T(b) + T(c) +K
  - **不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式。**

- 假设对n个元素进行归并排序需要的时间是T(n)，两个子数组排序的时间都是T(n/2)，merge()函数合并两个有序子数组的时间复杂度是O(n)，

  ```
  T(n) = 2*T(n/2) + n
       = 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n
       = 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n
       = 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n
       ......
       = 2^k * T(n/2^k) + k * n
       ......
  ```

- 当T(n/2^k)=T(1)时，也就是n/2^k=1，故T(n)=O(nlogn)，所以归并排序的时间复杂度是O(nlogn)。最好、最坏、平均都是

### 快速排序（QuickSort）

- 从排序数组中下标从p到r之间选择任意一个数据作为pivot（分区点）

- 我们遍历p到r之间的数据，将小于pivot的放到左边，将大于pivot的放到右边，将pivot放到中间。经过这一步骤之后，数组p到r之间的数据就被分成了三个部分，前面p到q-1之间都是小于pivot的，中间是pivot，后面的q+1到r之间是大于pivot的。

- 根据分治、递归的处理思想，我们可以用递归排序下标从p到q-1之间的数据和下标从q+1到r之间的数据，直到区间缩小为1，就说明所有的数据都有序了。

  ```
  // 快速排序，A是数组，n表示数组的大小
  quick_sort(A, n) {
    quick_sort_c(A, 0, n-1)
  }
  // 快速排序递归函数，p,r为下标
  quick_sort_c(A, p, r) {
    if p >= r then return
    
    q = partition(A, p, r) // 获取分区点
    quick_sort_c(A, p, q-1)
    quick_sort_c(A, q+1, r)
  }
  ```

- partition()分区函数可以很简单，随机选择一个元素作为pivot（一般可以选择p到r区间的最后一个元素），然后额外申请两个临时数组，数组p到r区间中小于pivot的元素放第一个数组，大于的放第二个数组，但是这样需要很多额外空间，不是原地排序

- 原地排序的分区函数实现如下

  ```
  partition(A, p, r) {
    pivot := A[r] // 选择最后一个元素作为pivot
    i := p
    for j := p to r-1 do {
      if A[j] < pivot {
        swap A[i] with A[j]
        i := i+1
      }
    }
    swap A[i] with A[r]
    return i
  ```

  - 这里的处理有点类似选择排序。我们通过游标i把A[p...r-1]分成两部分。A[p...i-1]的元素都是小于pivot的，我们暂且叫它“已处理区间”，A[i...r-1]是“未处理区间”。我们每次都从未处理的区间A[i...r-1]中取一个元素A[j]，与pivot对比，如果小于pivot，则将其加入到已处理区间的尾部，也就是A[i]的位置。

  - 交换，在O(1)的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将A[i]与A[j]交换，就可以在O(1)时间复杂度内将A[j]放到下标为i的位置。

  ![image-20220119002248240](https://tva1.sinaimg.cn/large/008i3skNly1gyiad7nol2j30z30u0ju5.jpg)

- 不稳定，因为有交换
- 最好时间复杂度：每次分区如果能把数组分成大小接近的两个子区间，那么快排的时间复杂度与归并排序类似，都是O(nlogn），可以通过合理选择pivot从而保证大部分时间都是O(nlogn)
  - 三数取中法：我们从区间的首、尾、中间，分别取出一个数，然后对比大小，取这3个数的中间值作为分区点。但当排序的数组比较大，那可能要五数取中、十数取中了
  - 随机法：从概率来看，平均的时间复杂度也比较好
- 最坏时间复杂度：分区极不均匀，退化成O(n^2)，但只有在极端情况才会出现

- 归并排序vs快排：归并排序的处理过程是**由下到上**的，先处理子问题，然后再合并。而快排正好相反，它的处理过程是**由上到下**的，先分区，然后再处理子问题。归并排序虽然是稳定的、时间复杂度为O(nlogn)的排序算法，但是它是非原地排序算法
- 快排用递归实现，要警惕堆栈溢出，可以设置堆栈阈值，超过则终止递归，也可以用栈来手动模拟递归
- 快排可以解决TopK问题

### 桶排序（Bucket sort）

- 核心思想是将要排序的数据分到几个有序的桶里，每个桶里的数据再单独进行排序。桶内排完序之后，再把每个桶里的数据按照顺序依次取出，组成的序列就是有序的了。
- 如果要排序的数据有n个，我们把它们均匀地划分到m个桶内，每个桶里就有k=n/m个元素。每个桶内部使用快速排序，时间复杂度为O(k * logk)。m个桶排序的时间复杂度就是O(m * k * logk)，因为k=n/m，所以整个桶排序的时间复杂度就是O(n*log(n/m))。当桶的个数m接近数据个数n时，log(n/m)就是一个非常小的常量，这个时候桶排序的时间复杂度接近O(n)。
- 苛刻的适用场景：能轻易划分到桶、桶有天然的大小顺序、数据在各个桶中比较均匀（极端场景是所有数据都在一个桶中，就退化成了O(nlogn)）
- **桶排序比较适合用在外部排序中**。所谓的外部排序就是数据存储在外部磁盘中，数据量比较大，内存有限，无法将数据全部加载到内存中。
- 可以解决给一群人的年龄排序、给高考成绩排序

### 计数排序（Counting sort）

- 我个人觉得，**计数排序其实是桶排序的一种特殊情况**。当要排序的n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划分成k个桶。每个桶内的数据值都是相同的，省掉了桶内排序的时间。

- 似乎计数排序与桶排序的差别仅在桶的粒度上，但是可以借助另一个数组来巧妙实现排序
- **计数排序只能用在数据范围不大的场景中，如果数据范围k比要排序的数据n大很多，就不适合用计数排序了。而且，计数排序只能给非负整数排序，如果要排序的数据是其他类型的，要将其在不改变相对大小的情况下，转化为非负整数。**

### 基数排序（Radix sort）

- 将数字按照个位数开始排序，然后再按照十位数来排序，以此类推
- 注意，这里按照每位来排序的排序算法要是稳定的，否则这个实现思路就是不正确的。因为如果是非稳定排序算法，那最后一次排序只会考虑最高位的大小顺序，完全不管其他位的大小关系，那么低位的排序就完全没有意义了。
- 根据每一位来排序，我们可以用刚讲过的桶排序或者计数排序，它们的时间复杂度可以做到O(n)。如果要排序的数据有k位，那我们就需要k次桶排序或者计数排序，总的时间复杂度是O(k*n)

- **基数排序对要排序的数据是有要求的，需要可以分割出独立的“位”来比较，而且位之间有递进的关系，如果a数据的高位比b数据大，那剩下的低位就不用比较了。除此之外，每一位的数据范围不能太大，要可以用线性排序算法来排序，否则，基数排序的时间复杂度就无法做到O(n)了**。

- 可以解决给电话号码排序的问题

- 假设我们现在需要对D，a，F，B，c，A，z这个字符串进行排序，要求将其中所有小写字母都排在大写字母的前面，但小写字母内部和大写字母内部不要求有序。比如经过排序之后为a，c，z，D，F，B，A，这个如何来实现呢？如果字符串中存储的不仅有大小写字母，还有数字。要将小写字母的放到前面，大写字母放在最后，数字放在中间，不用排序算法，又该怎么解决呢？

  将D，a，F，B，c，A，z转换成ASCII码。小于等于90的为第一组（大写字母）。大于等于97的为第二组（小写字母），最后第二组数据append到第一组以后即可。数字亦同理

### 排序优化

- Java语言采用堆排序实现排序函数，C语言使用快速排序实现排序函数。

- 归并排序因为不是原地排序，所以用的地方并不多


**Glibc的qsort()**

- **优先使用归并排序来排序输入数据**，空间换时间

- **排序的数据量比较大的时候，qsort()会改为用快速排序算法来排序**

- 三数取中法选择pivot

- 手动模拟递归，防止堆栈溢出

- 当要排序的区间中，元素的个数小于等于4时，qsort()就退化为插入排序

  > 在小规模数据面前，**O(n****2****)时间复杂度的算法并不一定比O(nlogn)的算法执行时间长**

### 二分查找

- **二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小为之前的一半，直到找到要查找的元素，或者区间被缩小为0**。

- 时间复杂度就是O(logn)

  > O(logn)这种**对数时间复杂度**有时效率比O(1)还高！因为logn可以快速缩小数量级

- 十个二分九个错，容易理解的写法才是最好的，不用可以追求优雅简洁

- **最简单的情况**就是**有序数组中不存在重复元素**

  ```java
  public int bsearch(int[] a, int n, int value) {
    int low = 0;
    int high = n - 1;
  
    while (low <= high) {
      int mid = (low + high) / 2;
      if (a[mid] == value) {
        return mid;
      } else if (a[mid] < value) {
        low = mid + 1;
      } else {
        high = mid - 1;
      }
    }
  
    return -1;
  }
  
  // 二分查找的递归实现
  public int bsearch(int[] a, int n, int val) {
    return bsearchInternally(a, 0, n - 1, val);
  }
  
  private int bsearchInternally(int[] a, int low, int high, int value) {
    if (low > high) return -1;
  
    int mid =  low + ((high - low) >> 1);
    if (a[mid] == value) {
      return mid;
    } else if (a[mid] < value) {
      return bsearchInternally(a, mid+1, high, value);
    } else {
      return bsearchInternally(a, low, mid-1, value);
    }
  }
  
  ```

- 容易出错的三个地方：

  - 循环退出条件，注意是low<=high
  - mid的取值，mid=(low+high)/2有可能溢出，改成low+(high-low)/2，追求性能的话，可以用low+((high-low)>>1)
  - low和high的更新，low=mid+1，high=mid-1

- 二分法局限
  - 必须是顺序表结构，即数组，链表不支持随机下标访问，所以不能用二分法
  - 只能查有序数据，二分查找只能用在插入、删除操作不频繁，一次排序多次查找的场景中
  - **数据量太小不适合二分查找。**直接遍历即可
  - **最后，数据量太大也不适合二分查找。**为了支持随机访问，要求内存空间连续，太大的数据用数组存储就比较吃力了，也就不能用二分查找了

- 查找第一个值等于给定值的元素

  - 如果mid等于0，那这个元素已经是数组的第一个元素，那它肯定是我们要找的；
  - 如果mid不等于0，但a[mid]的前一个元素a[mid-1]不等于value，那也说明a[mid]就是我们要找的第一个值等于给定值的元素。
  - 如果经过检查之后发现a[mid]前面的一个元素a[mid-1]也等于value，那说明此时的a[mid]肯定不是我们要查找的第一个值等于给定值的元素。那我们就更新high=mid-1，因为要找的元素肯定出现在[low, mid-1]之间。

  ```java
  public int bsearch(int[] a, int n, int value) {
    int low = 0;
    int high = n - 1;
    while (low <= high) {
      int mid =  low + ((high - low) >> 1);
      if (a[mid] > value) {
        high = mid - 1;
      } else if (a[mid] < value) {
        low = mid + 1;
      } else {
        if ((mid == 0) || (a[mid - 1] != value)) return mid;
        else high = mid - 1;
      }
    }
    return -1;
  }
  ```

- 查找最后一个值等于给定值的元素

  - 如果a[mid]这个元素已经是数组中的最后一个元素了，那它肯定是我们要找的；
  - 如果a[mid]的后一个元素a[mid+1]不等于value，那也说明a[mid]就是我们要找的最后一个值等于给定值的元素。
  - 如果我们经过检查之后，发现a[mid]后面的一个元素a[mid+1]也等于value，那说明当前的这个a[mid]并不是最后一个值等于给定值的元素。我们就更新low=mid+1，因为要找的元素肯定出现在[mid+1, high]之间。

  ```java
  public int bsearch(int[] a, int n, int value) {
    int low = 0;
    int high = n - 1;
    while (low <= high) {
      int mid =  low + ((high - low) >> 1);
      if (a[mid] > value) {
        high = mid - 1;
      } else if (a[mid] < value) {
        low = mid + 1;
      } else {
        if ((mid == n - 1) || (a[mid + 1] != value)) return mid;
        else low = mid + 1;
      }
    }
    return -1;
  }
  ```

- 查找第一个大于等于给定值的元素

  ```java
  public int bsearch(int[] a, int n, int value) {
    int low = 0;
    int high = n - 1;
    while (low <= high) {
      int mid =  low + ((high - low) >> 1);
      if (a[mid] >= value) {
        if ((mid == 0) || (a[mid - 1] < value)) return mid;
        else high = mid - 1;
      } else {
        low = mid + 1;
      }
    }
    return -1;
  }
  ```

- 查找最后一个小于等于给定值的元素

  ```java
  public int bsearch7(int[] a, int n, int value) {
    int low = 0;
    int high = n - 1;
    while (low <= high) {
      int mid =  low + ((high - low) >> 1);
      if (a[mid] > value) {
        high = mid - 1;
      } else {
        if ((mid == n - 1) || (a[mid + 1] > value)) return mid;
        else low = mid + 1;
      }
    }
    return -1;
  }
  ```


## 跳表（Skip list）

- 数组可以支持二分查找，但是链表不行，但可以对链表稍加改造，形成跳表，这样就可以支持二分查找了，其实跳表就是给普通链表加了索引

  ​	![image-20220120200113286](https://tva1.sinaimg.cn/large/008i3skNgy1gyke1p0aa3j31600nq0ux.jpg)

- 跳表的查找、插入、删除的时间复杂度为O(logn)，跳表的（额外）空间复杂度为O(n)

- 不断插入数据，索引可能会失效，极端情况会退化成单链表
  - 红黑树、AVL树这样平衡二叉树是通过左右旋保证左右子树的大小平衡，跳表是通过随机函数维护平衡性
  - 跳表通过一个随机函数，来决定将这个结点插入到哪几级索引中，比如随机函数生成了值K，那我们就将这个结点添加到第一级到第K级这K级索引中
  - 随机函数的选择很有讲究，从概率上来讲，能够保证跳表的索引大小和数据大小平衡性，不至于性能过度退化。
- **Redis使用跳表来实现有序集合**，之所以不用红黑树，是因为跳表还支持按照区间查找（只需要找到在跳表中往后遍历即可），而且跳表更容易编码与理解，通过改变索引构建策略来平衡执行效率和内存消耗
- 很多编程语言中的Map类型都是通过红黑树来实现的。我们做业务开发的时候，直接拿来用就可以，而跳表需要自己实现

## 散列表（Hash Table）

- **散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说，如果没有数组，就没有散列表。**

  ![image-20220120201030049](https://tva1.sinaimg.cn/large/008i3skNgy1gykebavpupj314c0qgq4s.jpg)

- 散列表用的就是数组支持按照下标随机访问的时候，时间复杂度是O(1)的特性。我们通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。当我们按照键值查询元素时，我们用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。

- 散列函数有可能将两个不同的key映射到同一个槽（slot）中，需要解决散列冲突

- 开放寻址法

  - **线性探测**（Linear Probing）：如果冲突，则往后线性寻找
  - **二次探测**（Quadratic probing）：如果冲突，则往后寻找的步长是原来的二次方
  - **双重散列**（Double hashing）：我们先用第一个散列函数，如果计算得到的存储位置已经被占用，再用第二个散列函数，依次类推，直到找到空闲的存储位置。
  - 注意：开放寻址法对于删除操作不能物理删除，只能逻辑删除
  - **当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是Java中的ThreadLocalMap使用开放寻址法解决散列冲突的原因**。

- 链表法

  ![image-20220120201654561](https://tva1.sinaimg.cn/large/008i3skNly1gykehyze3gj314k0natah.jpg)

  - 在散列表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表
  - 当插入的时候，我们只需要通过散列函数计算出对应的散列槽位，将其插入到对应链表中即可，所以插入的时间复杂度是O(1)
  - 当查找、删除一个元素时，我们同样通过散列函数计算出对应的槽，然后遍历链表查找或者删除（为了提高查找效率，可以用红黑树代替单链表）
  - **比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，支持更多的优化策略，比如用红黑树代替链表**。

- 散列表碰撞攻击：故意构造数据使得都大量数据散列到同一个槽里，使得查找复杂度急剧退化，从而消耗大量CPU资源，使得系统无法响应，达到拒绝服务攻击（DoS）的目的
- **散列函数的设计不能太复杂**，否则会消耗计算时间。**散列函数生成的值要尽可能随机并且均匀分布**

- **装载因子**（load factor） = 填入表中的元素个数/散列表的长度，当装载因子太大时，可以进行扩容：申请内存空间，重新计算哈希位置，并且搬移数据，时间复杂度是O(n)，但是可以均摊到前面的O(1)时间复杂度的插入操作上

- 如果数据量很大，一次性扩容可能很慢，我们可以将扩容操作穿插在插入操作的过程中：
  - 我们将新数据插入新散列表中，并且从老的散列表中拿出一个数据放入到新散列表
  - 对于查询操作，为了兼容了新、老散列表中的数据，我们先从新散列表中查找，如果没有找到，再去老的散列表中查找。
  - 这种实现方式，任何情况下，插入一个数据的时间复杂度都是O(1)。

- 工业级散列表举例分析

  - HashMap默认的初始大小是16，如果事先知道大概的数据量有多大，可以通过修改默认初始大小，减少动态扩容的次数

  - 最大装载因子默认是0.75，超过会启动扩容，每次扩容的空间是原来的两倍

  - 采用链表法解决散列冲突，而当链表长度太长（默认超过8）时，链表就转换为红黑树。我们可以利用红黑树快速增删改查的特点，提高HashMap的性能。当红黑树结点个数少于6个的时候，又会将红黑树转化为链表

  - 散列函数

    ```java
    int hash(Object key) {
        int h = key.hashCode()；
        return (h ^ (h >>> 16)) & (capicity -1); //capicity表示散列表的大小
    }
    ```

- 散列表经常与链表结合起来使用

  - LRU，查找元素时用散列表可以实现O(1)查找
  - Redis有序集合，key存在散列表里面，删除、查找一个成员对象的时间复杂度就变成了O(1)
  - Java中的LinkedHashMap：按照访问时间排序，其实就是个LRU，**LinkedHashMap是通过双向链表和散列表这两种数据结构组合实现的。LinkedHashMap中的“Linked”实际上是指的是双向链表，并非指用链表法解决散列冲突**。
  - 散列表这种数据结构虽然支持非常高效的数据插入、删除、查找操作，但是散列表中的数据都是通过散列函数打乱之后无规律存储的。也就说，它无法支持按照某种顺序快速地遍历数据。如果希望按照顺序遍历散列表中的数据，那我们需要将散列表中的数据拷贝到数组中，然后排序，再遍历。

## 哈希算法

- 从哈希值不能反向推导出原始数据（所以哈希算法也叫单向哈希算法）；
- 对输入数据非常敏感，哪怕原始数据只修改了一个Bit，最后得到的哈希值也大不相同；
- 散列冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小；
- 哈希算法的执行效率要尽量高效，针对较长的文本，也能快速地计算出哈希值
- **为什么哈希算法无法做到零冲突**：鸽巢原理，11个鸽子10个巢，肯定有冲突的
- 应用一：安全加密
  - 最常用：**MD5**（MD5 Message-Digest Algorithm，MD5消息摘要算法）和**SHA**（Secure Hash Algorithm，安全散列算法）。
  - 除了这两个之外，当然还有很多其他加密算法，比如**DES**（Data Encryption Standard，数据加密标准）、**AES**（Advanced Encryption Standard，高级加密标准）
  - 没有绝对安全的加密。越复杂、越难破解的加密算法，需要的计算时间也越长

- 应用二：唯一标识
  - 比如图片的信息摘要，通过哈希算法生成唯一标识，可以快速判断是否存在，如果存在（有相同的哈希值），则去图库做全量对比（与布隆过滤器有点像
- 应用三：数据校验
  - 网络上下载的数据有可能被篡改，数据提供方可以提供一个哈希值，数据下载方下载后将数据进行哈希，得到的哈希值与原来的哈希值做对比

- 应用四：散列函数
  - 散列函数对于散列算法冲突的要求要低很多。
  - 是否能反向解密也并不关心
  - 散列函数执行的快慢，也会影响散列表的性能，所以，散列函数用的散列算法一般都比较简单，比较追求效率。

- 应用五：负载均衡
  - 负载均衡算法有很多，比如轮询、随机、加权轮询
  - 会话粘滞（session sticky）的负载均衡算法：同一个客户端上，在一次会话中的所有请求都路由到同一个服务器
  - 如果简单维护一个映射关系表，则当客户端很多时，消耗空间大，且当客户端上下线、扩缩容欧惠导致映射失效
  - **我们可以通过哈希算法，对客户端IP地址或者会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号**

- 应用六：数据分片

  - 假如我们有1T的日志文件，这里面记录了用户的搜索关键词，我们想要快速统计出每个关键词被搜索的次数，该怎么做呢？
    - **我们可以先对数据进行分片，然后采用多台机器处理的方法，来提高处理速度**。
    - 具体的思路是这样的：为了提高处理的速度，我们用n台机器并行处理。我们从搜索记录的日志文件中，依次读出每个搜索关键词，并且通过哈希函数计算哈希值，然后再跟n取模，最终得到的值，就是应该被分配到的机器编号。
    - 这样，哈希值相同的搜索关键词就被分配到了同一个机器上。也就是说，同一个搜索关键词会被分配到同一个机器上。每个机器会分别计算关键词出现的次数，最后合并起来就是最终的结果。
    - 实际上，这里的处理过程也是MapReduce的基本设计思想。

  - 假设现在我们的图库中有1亿张图片，如何快速判断图片是否在图库中
    - 我们同样可以对数据进行分片，然后采用多机处理。我们准备n台机器，让每台机器只维护某一部分图片对应的散列表。我们每次从图库中读取一个图片，计算唯一标识，然后与机器个数n求余取模，得到的值就对应要分配的机器编号，然后将这个图片的唯一标识和图片路径发往对应的机器构建散列表。
    - 当我们要判断一个图片是否在图库中的时候，我们通过同样的哈希算法，计算这个图片的唯一标识，然后与机器个数n求余取模。假设得到的值是k，那就去编号k的机器构建的散列表中查找。

  - 实际上，针对这种海量数据的处理问题，我们都可以采用多机分布式处理。借助这种分片的思路，可以突破单机内存、CPU等资源的限制。

- 应用七：分布式存储

  - 存储节点是分布式的，如何决定数据放到哪个机器上
  - 我们可以借用前面数据分片的思想，即通过哈希算法对数据取哈希值，然后对机器个数取模，这个最终值就是应该存储的缓存机器编号。
  - 然而，我们需要一种方法，使得在新加入一个机器后，并不需要做大量的数据搬移。这时候，**一致性哈希算法**就要登场了。
  - 假设我们有k个机器，数据的哈希值的范围是[0, MAX]。我们将整个范围划分成m个小区间（m远大于k），每个机器负责m/k个小区间。当有新机器加入的时候，我们就将某几个小区间的数据，从原来的机器中搬移到新的机器中。这样，既不用全部重新哈希、搬移数据，也保持了各个机器上数据数量的均衡。它还会借助一个虚拟的环和虚拟结点，更加优美地实现出来

## 二叉树

- **高度**（Height）、**深度**（Depth）、**层**（Level）
  - 在我们的生活中，“高度”这个概念，其实就是从下往上度量，比如我们要度量第10层楼的高度、第13层楼的高度，起点都是地面。
  - “深度”这个概念在生活中是从上往下度量的，比如水中鱼的深度，是从水平面开始度量的
  - “层数”跟深度的计算类似，不过，计数起点是1，也就是说根节点位于第1层。

- 二叉树（Binary Tree）

  - 除了叶子节点之外，每个节点都有左右两个子节点，这种二叉树就叫做**满二叉树**。
  - 叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做**完全二叉树**

- 存储一棵二叉树，我们有两种方法，一种是基于指针或者引用的二叉链式存储法，一种是基于数组的顺序存储法。

  - 链表存储法很容易理解，需要额外的左右子树的指针
  - 基于数组的顺序存储法：如果节点X存储在数组中下标为i的位置，下标为2 * i 的位置存储的就是左子节点，下标为2 * i + 1的位置存储的就是右子节点。反过来，下标为i/2的位置存储就是它的父节点。通过这种方式，我们只要知道根节点存储的位置（一般情况下，为了方便计算子节点，根节点会存储在下标为1的位置），这样就可以通过下标计算，把整棵树都串起来。

  ![image-20220121121318573](https://tva1.sinaimg.cn/large/008i3skNly1gyl6541lgij313u0m2dgr.jpg)

  - 如果某棵二叉树是一棵完全二叉树，那用数组存储无疑是最节省内存的一种方式。因为数组的存储方式并不需要像链式存储法那样，要存储额外的左右子节点的指针

- 二叉树的遍历

  - 前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。

  - 中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。

  - 后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。

  - 二叉树遍历的时间复杂度是O(n)

    ```c
    void preOrder(Node* root) {
      if (root == null) return;
      print root // 此处为伪代码，表示打印root节点
      preOrder(root->left);
      preOrder(root->right);
    }
    
    void inOrder(Node* root) {
      if (root == null) return;
      inOrder(root->left);
      print root // 此处为伪代码，表示打印root节点
      inOrder(root->right);
    }
    
    void postOrder(Node* root) {
      if (root == null) return;
      postOrder(root->left);
      postOrder(root->right);
      print root // 此处为伪代码，表示打印root节点
    }
    ```

- 二叉查找树（Binary Search Tree）

  - **中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是O(n)，非常高效**，所以又叫二叉排序树

  - 查找：我们先取根节点，如果它等于我们要查找的数据，那就返回。如果要查找的数据比根节点的值小，那就在左子树中递归查找；如果要查找的数据比根节点的值大，那就在右子树中递归查找。

    ```java
    public class BinarySearchTree {
      private Node tree;
    
      public Node find(int data) {
        Node p = tree;
        while (p != null) {
          if (data < p.data) p = p.left;
          else if (data > p.data) p = p.right;
          else return p;
        }
        return null;
      }
    
      public static class Node {
        private int data;
        private Node left;
        private Node right;
    
        public Node(int data) {
          this.data = data;
        }
      }
    }
    ```

  - 插入：新插入的数据一般都是在叶子节点上，所以我们只需要从根节点开始，依次比较要插入的数据和节点的大小关系。如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插到右子节点的位置；如果不为空，就再递归遍历右子树，查找插入位置。右边同理

    ```java
    public void insert(int data) {
      if (tree == null) {
        tree = new Node(data);
        return;
      }
    
      Node p = tree;
      while (p != null) {
        if (data > p.data) {
          if (p.right == null) {
            p.right = new Node(data);
            return;
          }
          p = p.right;
        } else { // data < p.data
          if (p.left == null) {
            p.left = new Node(data);
            return;
          }
          p = p.left;
        }
      }
    }
    ```

  - 删除：分三种情况处理

    - 第一种情况是，如果要删除的节点没有子节点，我们只需要直接将父节点中，指向要删除节点的指针置为null。比如图中的删除节点55。

    - 第二种情况是，如果要删除的节点只有一个子节点（只有左子节点或者右子节点），我们只需要更新父节点中，指向要删除节点的指针，让它指向要删除节点的子节点就可以了。比如图中的删除节点13。

    - 第三种情况是，如果要删除的节点有两个子节点，这就比较复杂了。我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了），所以，我们可以应用上面两条规则来删除这个最小节点

      ![image-20220121122104423](https://tva1.sinaimg.cn/large/008i3skNly1gyl6d733gpj31480m4407.jpg)

      ```java
      public void delete(int data) {
        Node p = tree; // p指向要删除的节点，初始化指向根节点
        Node pp = null; // pp记录的是p的父节点
        while (p != null && p.data != data) {
          pp = p;
          if (data > p.data) p = p.right;
          else p = p.left;
        }
        if (p == null) return; // 没有找到
      
        // 要删除的节点有两个子节点
        if (p.left != null && p.right != null) { // 查找右子树中最小节点
          Node minP = p.right;
          Node minPP = p; // minPP表示minP的父节点
          while (minP.left != null) {
            minPP = minP;
            minP = minP.left;
          }
          p.data = minP.data; // 将minP的数据替换到p中
          p = minP; // 下面就变成了删除minP了
          pp = minPP;
        }
      
        // 删除节点是叶子节点或者仅有一个子节点
        Node child; // p的子节点
        if (p.left != null) child = p.left;
        else if (p.right != null) child = p.right;
        else child = null;
      
        if (pp == null) tree = child; // 删除的是根节点
        else if (pp.left == p) pp.left = child;
        else pp.right = child;
      }
      ```

      - 还有个非常简单、取巧的方法，就是单纯将要删除的节点标记为“已删除”，但是并不真正从树中将这个节点去掉。这样原本删除的节点还需要存储在内存中，比较浪费内存空间，但是删除操作就变得简单了

- 支持重复数据的二叉查找树
  - 二叉查找树中存储的，是一个包含很多字段的对象。我们利用对象的某个字段作为键值（key）来构建二叉查找树。我们把对象中的其他字段叫作卫星数据。
  - 如果两个对象的键值相同，有两种处理方法
    - 使用链表或者支持动态扩容的数组等结构，将他们存储到同一个节点上
    - 每个节点仍然只存储一个数据。在查找插入位置的过程中，如果碰到一个节点的值，与要插入数据的值相同，我们就将这个要插入的数据放到这个节点的右子树，也就是说，把这个新插入的数据当作大于这个节点的值来处理。当要查找数据的时候，遇到值相同的节点，我们并不停止查找操作，而是继续在右子树中查找，直到遇到叶子节点，才停止。这样就可以把键值等于要查找值的所有节点都找出来。对于删除操作，我们也需要先查找到每个要删除的节点，然后再按前面讲的删除操作的方法，依次删除。

- 极度不平衡的二叉查找树，会退化成链表，查找的时间复杂度退化成O(n)

- 二叉查找树与散列表的比较
  - 第一，散列表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在O(n)的时间复杂度内，输出有序的数据序列。
  - 第二，散列表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在O(logn)。
  - 第三，笼统地来说，尽管散列表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比logn小，所以实际的查找速度可能不一定比O(logn)快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。
  - 第四，散列表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。

## 红黑树

- 发明平衡二叉查找树这类数据结构的初衷是，解决普通二叉查找树在频繁的插入、删除等动态更新的情况下，出现时间复杂度退化的问题。
- 所以，**平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。**

- 平衡二叉查找树有很多，比如AVL树、Splay Tree（伸展树、Treap（树堆），但是红黑树是最出名的，出镜率甚至还要高于平衡二叉查找树
- 红黑树R-B Tree定义，每个节点是黑色或者红色的
  - 根节点是黑色的；
  - 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据；
  - 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
  - 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点；

- 红黑树的高度近似log2n，所以它是近似平衡，插入、删除、查找操作的时间复杂度都是O(logn)。AVL树是严格平衡的，所以在维护平衡的成本上，要比AVL树要低。
- 所以，红黑树的插入、删除、查找各种操作性能都比较稳定，工业级应用都用它，但编写代码难度较高，可以用跳表代替它

## 递归树

- **借助递归树来分析递归算法的时间复杂度**。

![image-20220121125639194](https://tva1.sinaimg.cn/large/008i3skNly1gyl7e7c0xaj314y0pswhc.jpg)

![image-20220121125648919](https://tva1.sinaimg.cn/large/008i3skNly1gyl7edcat7j314y0tajv3.jpg)

## 堆和堆排序

- 堆是一个完全二叉树；堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值。大于等于的叫大顶堆，小于等于叫小顶堆

- 完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。所以堆一般用数组实现

- 插入操作：

  - 插入后必须满足堆的特性，所以我们就需要进行调整，让其重新满足堆的特性，这个过程我们起了一个名字，就叫做**堆化**（heapify）。

    ![image-20220121143759688](https://tva1.sinaimg.cn/large/008i3skNly1gylabn18s2j314i0ruwh1.jpg)

  - 插入用**从下到上**的堆化比较方便，先把待插入元素放到数组末尾，然后与它的父节点进行比较，进行交换，直到满足堆的特性

    ```c
    public class Heap {
      private int[] a; // 数组，从下标1开始存储数据
      private int n;  // 堆可以存储的最大数据个数
      private int count; // 堆中已经存储的数据个数
    
      public Heap(int capacity) {
        a = new int[capacity + 1];
        n = capacity;
        count = 0;
      }
    
      public void insert(int data) {
        if (count >= n) return; // 堆满了
        ++count;
        a[count] = data;
        int i = count;
        while (i/2 > 0 && a[i] > a[i/2]) { // 自下往上堆化
          swap(a, i, i/2); // swap()函数作用：交换下标为i和i/2的两个元素
          i = i/2;
        }
      }
     }
    ```

- 删除堆顶操作：

  - 删除堆顶元素后，用**从上到下**的堆化比较方便，先把数组末尾的元素放到堆顶，然后比较与交换

    ![image-20220121143749045](https://tva1.sinaimg.cn/large/008i3skNly1gylabgms67j313l0u0q61.jpg)

  - 因为都是交换操作，这样不会出现**空洞**

    ```c
    public void removeMax() {
      if (count == 0) return -1; // 堆中没有数据
      a[1] = a[count];
      --count;
      heapify(a, count, 1);
    }
    
    private void heapify(int[] a, int n, int i) { // 自上往下堆化
      while (true) {
        int maxPos = i;
        if (i*2 <= n && a[i] < a[i*2]) maxPos = i*2;
        if (i*2+1 <= n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;
        if (maxPos == i) break;
        swap(a, i, maxPos);
        i = maxPos;
      }
    }
    ```

- 堆化的时间复杂度与树的高度成正比，所以也是O(logn)，插入与删除的主要逻辑就是堆化，所以它们俩的时间复杂度都是O(logn)

- 堆排序是一种原地的、时间复杂度为O(nlog n)的排序算法，还是原地排序，非常优秀，但不是稳定排序

  - 建堆，有两种方式

    - 从前往后，不断插入，每次插入就堆化，从下往上堆化

    - 从后往前，从第一个非叶子节点开始（下标是n/2+1~n的节点都是叶子节点，不需要堆化），从上往下堆化，这种情况的建堆复杂度是O(n)

      ```c
      private static void buildHeap(int[] a, int n) {
        for (int i = n/2; i >= 1; --i) {
          heapify(a, n, i);
        }
      }
      
      private static void heapify(int[] a, int n, int i) {
        while (true) {
          int maxPos = i;
          if (i*2 <= n && a[i] < a[i*2]) maxPos = i*2;
          if (i*2+1 <= n && a[maxPos] < a[i*2+1]) maxPos = i*2+1;
          if (maxPos == i) break;
          swap(a, i, maxPos);
          i = maxPos;
        }
      }
      ```

      

      ![image-20220121144853049](https://tva1.sinaimg.cn/large/008i3skNly1gylamzztsnj30u018padh.jpg)

  - 排序

    - 堆顶元素即为最大元素，将它与最后一个元素交换，那最大元素就到了下标为n的位置

    - 这个过程有点类似上面讲的“删除堆顶元素”的操作，然后再通过堆化操作，将剩下的n-1的元素重新构建

    - 再取堆顶元素，与下标n-1的元素交换，一直重复，直到堆只剩堆顶这最后一个元素（即为最小的元素），排序即完成

      ```c
      // n表示数据的个数，数组a中的数据从下标1到n的位置。
      public static void sort(int[] a, int n) {
        buildHeap(a, n);
        int k = n;
        while (k > 1) {
          swap(a, 1, k);
          --k;
          heapify(a, k, 1);
        }
      }
      ```

- 快速排序vs堆排序

  - 堆排序是跳着访问的，对CPU缓存不友好
  - 同样的数据，堆排序的交换次数要大于快速排序

- 堆的应用一：优先级队列
  - 堆可以看做优先级队列，优先级最高的最先出队，Java的PriorityQueue，C++的priority_queue
  - 合并有序小文件：假设我们有100个小文件，每个文件的大小是100MB，每个文件中存储的都是有序的字符串。我们希望将这些100个小文件合并成一个有序的大文件。
    - 我们将从小文件中取出来的字符串放入到小顶堆中，那堆顶的元素，也就是优先级队列队首的元素，就是最小的字符串。我们将这个字符串放入到大文件中，并将其从堆中删除。然后再从小文件中取出下一个字符串，放入到堆中。循环这个过程，就可以将100个小文件中的数据依次放入到大文件中。
  - 高性能定时器：我们按照任务设定的执行时间，将这些任务存储在优先级队列中，队列首部（也就是小顶堆的堆顶）存储的是最先执行的任务。

- 堆的应用二：利用堆求Top K
  - 针对静态数据，如何在一个包含n个数据的数组中，查找前K大数据呢？我们可以维护一个大小为K的小顶堆，顺序遍历数组，从数组中取出数据与堆顶元素比较。如果比堆顶元素大，我们就把堆顶元素删除，并且将这个元素插入到堆中；如果比堆顶元素小，则不做处理，继续遍历数组。这样等数组中的数据都遍历完之后，堆中的数据就是前K大数据了。遍历数组需要O(n)的时间复杂度，一次堆化操作需要O(logK)的时间复杂度，所以最坏情况下，n个元素都入堆一次，时间复杂度就是O(nlogK)。
  - 针对动态数据，我们可以先构造小顶堆，然后每当有数据插入，就与堆顶比较，如果大于则将堆顶元素删除，并且将新元素插入堆中，这样可以随时给出实时TopK的数据

- 堆的应用三：利用堆求中位数
  - 对于一组**静态数据**，中位数是固定的，我们可以先排序，第n/2的数据就是中位数
  - 但是对于**动态数据**，就要用到一个小顶堆和一个大顶堆，大顶堆中存储前半部分数据，小顶堆中存储后半部分数据，且小顶堆中的数据都大于大顶堆中的数据。
    - 如果有n个数据，n是偶数，我们从小到大排序，那前n/2个数据存储在大顶堆中，后n/2个数据存储在小顶堆中。这样，大顶堆中的堆顶元素就是我们要找的中位数。如果n是奇数，情况是类似的，大顶堆就存储n/2+1个数据，小顶堆中就存储n/2个数据。
    - 如果新加入的数据小于等于大顶堆的堆顶元素，我们就将这个新数据插入到大顶堆；否则，我们就将这个新数据插入到小顶堆。
    - 这时可能会出现不满足大小堆的元素个数约定，应该将元素多的那个堆的堆顶元素移动到另一个堆中
    - 插入数据因为需要涉及堆化，所以时间复杂度变成了O(logn)，但是求中位数我们只需要返回大顶堆的堆顶元素就可以了，所以时间复杂度就是O(1)。
  - 不仅可以用来求中位数，还可以求"如何快速求接口的99%响应时间"，我们维护两个堆，一个大顶堆，一个小顶堆。假设当前总数据的个数是n，大顶堆中保存`n*99%`个数据，小顶堆中保存`n*1%`个数据。大顶堆堆顶的数据就是我们要找的99%响应时间。

- 求10亿数据中的Top 10最频繁的搜索关键词
  - 我们创建10个空文件00，01，02，……，09。我们遍历这10亿个关键词，并且通过某个哈希算法对其求哈希值，然后哈希值同10取模，得到的结果就是这个搜索关键词应该被分到的文件编号。
  - 对这10亿个关键词分片之后，每个文件都只有1亿的关键词，去除掉重复的，可能就只有1000万个，每个关键词平均50个字节，所以总的大小就是500MB。1GB的内存完全可以放得下。
  - 我们针对每个包含1亿条搜索关键词的文件，散列表用于计数，堆用于求TOP10，分别求出Top 10，然后把这个10个Top 10放在一块，然后取这100个关键词中，出现次数最多的10个关键词，这就是这10亿数据中的Top 10最频繁的搜索关键词了。

## 图

- 图、顶点、边、有向图、入度、出度、带权图、**邻接矩阵**、稀疏图、**邻接表**（可以用红黑树代替链表，从而改善查找效率）、逆邻接表

- 以社交网络的微博为例，设计数据结构

  - 判断用户A是否关注了用户B；

    - 社交网络是稀疏图，所以用邻接表更省空间

  - 判断用户A是否是用户B的粉丝；

    - 除了邻接表，还需要逆邻接表

  - 根据用户名称的首字母排序，分页获取用户的粉丝列表；

    - 跳表，本来就是有序的，插入、删除、查找的时间复杂度都是O(n)，分页非常高效

    

## 深度优先搜索与广度优先搜索

- 广度优先搜索（Breadth-First-Search）：

  **visited**是用来记录已经被访问的顶点，用来避免顶点被重复访问。如果顶点q被访问，那相应的visited[q]会被设置为true。

  **queue**是一个队列，用来存储已经被访问、但相连的顶点还没有被访问的顶点。因为广度优先搜索是逐层访问的，也就是说，我们只有把第k层的顶点都访问完成之后，才能访问第k+1层的顶点。当我们访问到第k层的顶点的时候，我们需要把第k层的顶点记录下来，稍后才能通过第k层的顶点来找第k+1层的顶点。所以，我们用这个队列来实现记录的功能。

  **prev**用来记录搜索路径。当我们从顶点s开始，广度优先搜索到顶点t后，prev数组中存储的就是搜索的路径。不过，这个路径是反向存储的。prev[w]存储的是，顶点w是从哪个前驱顶点遍历过来的。比如，我们通过顶点2的邻接表访问到顶点3，那prev[3]就等于2。为了正向打印出路径，我们需要递归地来打印，你可以看下print()函数的实现方式。

- 广度优先搜索的时间复杂度是O(V+E)，其中，V表示顶点的个数，E表示边的个数

- 深度优先搜索（Depth-First-Search），递归调用栈的最大深度不会超过顶点的个数，所以总的空间复杂度就是O(V)，深度优先搜索算法的时间复杂度是O(E)，E表示边的个数

## 字符串匹配

- 定于：我们在字符串A中查找字符串B，那字符串A就是主串，字符串B就是模式串。我们把主串的长度记作n，模式串的长度记作m。因为我们是在主串中查找模式串，所以n>m。

### BF（Brute Force）

- **我们在主串中，检查起始位置分别是0、1、2....n-m且长度为m的n-m+1个子串，看有没有跟模式串匹配的**。
- 时间复杂度很高，是O(n*m)
- 但很常用，因为工程中m和n都很小，以及BF算法非常简单，容易排查

### RK（Rabin-Karp）

- 我们通过哈希算法对主串中的n-m+1个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了
- 不过，通过哈希算法计算子串的哈希值的时候，我们需要遍历子串中的每个字符。尽管模式串与子串比较的效率提高了，但是，算法整体的效率并没有提高。有没有方法可以提高哈希算法计算子串哈希值的效率呢？
- 我们假设要匹配的字符串的字符集中只包含K个字符，我们可以用一个K进制数来表示一个子串，这个K进制数转化成十进制数，作为子串的哈希值
- 这样的哈希算法，相邻两个子串的哈希值计算公式中有很大重复，可以加速计算，而且还可以用查表法事先计算K的m-1次方
- 整体的时间复杂度是O(n)
- 还有一个问题，哈希值可能超出整型数据，其实我们可以牺牲一下，允许哈希冲突，就可以把哈希值落在整型范围内了，当发现哈希值相等时，再匹配一下字符串本身就好了，但记得要把冲突概率控制得相对低一些

### BM（Boyer-Moore）算法

- 我们把模式串和主串的匹配过程，看作模式串在主串中不停地往后滑动。当遇到不匹配的字符时，BF算法和RK算法的做法是，模式串往后滑动一位，然后从模式串的第一个字符开始重新匹配
- 但其实，有一些规律可循，可以让模式串往后多滑动几位
- 构建坏规则和好规则，让模式串高效的滑动！

### KMP算法

- 比BM还难懂

## Trie字典树

- 树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。

- **Trie树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起**

- 其中，根节点不包含任何信息。每个节点表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串

  ![image-20220121171925181](https://tva1.sinaimg.cn/large/008i3skNly1gylezlsji6j30wn0u0wgy.jpg)

- 代码实现

  ```java
  class TrieNode {
    char data;
    TrieNode children[26]; // 假设都是小写英文字符
  }
  public class Trie {
    private TrieNode root = new TrieNode('/'); // 存储无意义字符
  
    // 往Trie树中插入一个字符串
    public void insert(char[] text) {
      TrieNode p = root;
      for (int i = 0; i < text.length; ++i) {
        int index = text[i] - 'a';
        if (p.children[index] == null) {
          TrieNode newNode = new TrieNode(text[i]);
          p.children[index] = newNode;
        }
        p = p.children[index];
      }
      p.isEndingChar = true;
    }
  
    // 在Trie树中查找一个字符串
    public boolean find(char[] pattern) {
      TrieNode p = root;
      for (int i = 0; i < pattern.length; ++i) {
        int index = pattern[i] - 'a';
        if (p.children[index] == null) {
          return false; // 不存在pattern
        }
        p = p.children[index];
      }
      if (p.isEndingChar == false) return false; // 不能完全匹配，只是前缀
      else return true; // 找到pattern
    }
  
    public class TrieNode {
      public char data;
      public TrieNode[] children = new TrieNode[26];
      public boolean isEndingChar = false;
      public TrieNode(char data) {
        this.data = data;
      }
    }
  }
  ```

- 如果要在一组字符串中，频繁地查询某些字符串，用Trie树会非常高效。构建Trie树的过程，需要扫描所有的字符串，时间复杂度是O(n)（n表示所有字符串的长度和）。但是一旦构建成功之后，后续的查询操作会非常高效。

- 当字符种类比较多时，Trie树有可能会消耗很多空间

  - 我们可以稍微牺牲一点查询的效率，将每个节点中的数组换成其他数据结构，来存储一个节点的子节点指针。用哪种数据结构呢？我们的选择其实有很多，比如有序数组、跳表、散列表、红黑树等。
  - 假设我们用有序数组，数组中的指针按照所指向的子节点中的字符的大小顺序排列。查询的时候，我们可以通过二分查找的方法，快速查找到某个字符应该匹配的子节点的指针。但是，在往Trie树中插入一个字符串的时候，我们为了维护数组中数据的有序性，就会稍微慢了点。

- 缩点优化，可以节省空间，但对编码要求较高

  ![image-20220121172541746](https://tva1.sinaimg.cn/large/008i3skNly1gylf65ia0pj313w0jkabg.jpg)

- Trie的缺点
  - 字符串中包含的字符集不能太大，不然空间消耗大
  - 要求前缀重合比较多，不然空间消耗大
  - 需要从零实现
  - Trie树用到了指针，对缓存不友好
  - Trie树的优势并不在于，用它来做动态集合数据的查找，因为，这个工作完全可以用更加合适的散列表或者红黑树来替代
- Trie最适合的场景：前缀匹配，比如搜索引擎的关键字提示功能，IDE的代码自动补全

## AC自动机：**：如何用多模式串匹配实现敏感词过滤功能？**

- 单模式串匹配算法，是在一个模式串和一个主串之间进行匹配，也就是说，在一个主串中查找一个模式串。多模式串匹配算法，就是在多个模式串和一个主串之间做匹配，也就是说，在一个主串中查找多个模式串。
- 除了Trie树，前面的算法都是单模式串匹配算法，AC自动机进一步优化了Trie的效率

- AC自动机算法，全称是Aho-Corasick算法。其实，Trie树跟AC自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟KMP算法之间的关系一样，只不过前者针对的是多模式串而已。所以，**AC自动机实际上就是在Trie树之上，加了类似KMP的next数组，只不过此处的next数组是构建在树上罢了**

##  贪心算法

- 证明贪心算法的关键是证明局部最优可以达成全局最优，但是证明很难，实际上，大部分能用贪心算法解决的问题，其正确性都是显而易见的

- 应用

  - 分糖果
  - 钱币找零
  - 区间覆盖
  - 霍夫曼编码

- 思考题

  - 在一个非负整数a中，我们希望从中移除k个数字，让剩下的数字值最小，如何选择移除哪k个数字呢？

    每次贪心的移除最大的

  - 假设有n个人等待被服务，但是服务窗口只有一个，每个人需要被服务的时间长度是不同的，如何安排被服务的先后顺序，才能让这n个人总的等待时间最短？

    每次让需要服务时间短的人优先

## 分治算法（divide and conquer）

- **分治算法是一种处理问题的思想，递归是一种编程技巧**。实际上，分治算法一般都比较适合用递归来实现
- 分治算法的递归实现中，每一层递归都会涉及这样三个操作：
  - 分解：将原问题分解成一系列子问题；
  - 解决：递归地求解各个子问题，若子问题足够小，则直接求解；
  - 合并：将子问题的结果合并成原问题。
- 分治算法能解决的问题，一般需要满足下面这几个条件：
  - 原问题与分解成的小问题具有相同的模式；
  - 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法；
  - 具有分解终止条件，也就是说，当问题足够小时，可以直接求解；
  - 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。
- 统计逆序对，修改归并算法即可
- MapReduce框架只是一个任务调度器，底层依赖GFS来存储数据，依赖Borg管理机器。一台机器过于低效，那我们就把任务拆分到多台机器上来处理。如果拆分之后的小任务之间互不干扰，独立计算，最后再将结果合并

## 回溯算法

- 它除了用来指导像深度优先搜索这种经典的算法设计之外，还可以用在很多实际的软件开发场景中，比如正则表达式匹配、编译原理中的语法分析等。
- 除此之外，很多经典的数学问题都可以用回溯算法解决，比如数独、八皇后、0-1背包、图的着色、旅行商问题、全排列

- 回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。

- 回溯算法非常适合用递归来实现，在实现的过程中，剪枝操作是提高回溯效率的一种技巧。利用剪枝，我们并不需要穷举搜索所有的情况，从而提高搜索效率。
- 回溯算法是个“万金油”。基本上能用的动态规划、贪心解决的问题，我们都可以用回溯算法解决。回溯算法相当于穷举搜索。穷举所有的情况，然后对比得到最优解

## 动态规划

- 动态规划比较适合用来求解最优问题，比如求最大值、最小值等等。它可以非常显著地降低时间复杂度，提高代码的执行效率。
- 大部分动态规划能解决的问题，都可以通过回溯算法来解决，只不过回溯算法解决起来效率比较低，时间复杂度是指数级的，动态规划是一种空间换时间的算法思想
- **多阶段决策最优解模型**：**最优子结构**、**无后效性**和**重复子问题**
  - 最优子结构：问题的最优解包含子问题的最优解，反过来说就是，我们可以通过子问题的最优解，推导出问题的最优解
  - 无后效性：我们只关心前面阶段的状态值，不关心这个状态是怎么一步一步推导出来的
  - 重复子问题：不同的决策序列，到达某个相同的阶段时，可能会产生重复的状态

- **状态转移方程是解决动态规划的关键。**如果我们能写出状态转移方程，那动态规划问题基本上就解决一大半了，有两种代码实现方法，一种是**递归加“备忘录”**，另一种是**迭代递推**。

- 动态规划vs分治算法

  - 在重复子问题这一点上，分治算法要求分割成的子问题，不能有重复子问题，而动态规划正好相反，动态规划之所以高效，就是因为回溯算法实现中存在大量的重复子问题。

- 动态规划vs贪心算法

  - 贪心算法实际上是动态规划算法的一种特殊情况。它更简洁，但限制大，因为需要局部最优产生全局最优

- 思考题：假设我们有几种不同币值的硬币v1，v2，……，vn（单位是元）。如果我们要支付w元，求最少需要多少个硬币。比如，我们有3种不同的硬币，1元、3元、5元，我们要支付9元，最少需要3个硬币（3个3元的硬币）。

  假设支付支付w元最少需要k硬币，则`d[w][k]=min{d[w-v1][k-1] + 1, d[w-v2][k-1] + 1, .... , d[w-vn][k-1] + 1}`

- 编辑距离：

  - 将一个字符串转化成另一个字符串，需要的最少编辑操作次数（比如增加一个字符、删除一个字符、替换一个字符），可以用来量化两个字符串之间的相似程度

  - 莱文斯坦距离的大小，表示两个字符串差异的大小；而最长公共子串的大小，表示两个字符串相似程度的大小。

    ```c
    // 莱文斯坦距离
    如果：a[i]!=b[j]，那么：min_edist(i, j)就等于：
    min(min_edist(i-1,j)+1, min_edist(i,j-1)+1, min_edist(i-1,j-1)+1)
    
    如果：a[i]==b[j]，那么：min_edist(i, j)就等于：
    min(min_edist(i-1,j)+1, min_edist(i,j-1)+1，min_edist(i-1,j-1))
    
    其中，min表示求三数中的最小值。     
     
    // 最长公共子串  
    如果：a[i]==b[j]，那么：max_lcs(i, j)就等于：
    max(max_lcs(i-1,j-1)+1, max_lcs(i-1, j), max_lcs(i, j-1))；
    
    如果：a[i]!=b[j]，那么：max_lcs(i, j)就等于：
    max(max_lcs(i-1,j-1), max_lcs(i-1, j), max_lcs(i, j-1))；
    
    其中max表示求三数中的最大值。
    
     
    ```

- 搜索引擎的实现：

  - 当用户在搜索框内，输入一个拼写错误的单词时，我们就拿这个单词与词库里编辑距离最小的单词展示给用户

  - 改进：取出编辑距离最小的TOP 10，再根据热门程度推荐给用户

  - 改进：多重编辑距离计算方法取最小编辑距离top10，再取交集

  - 改进：分析用户的搜索日志，找出是否有常用单词

  - 改进：个性化因素，个性推荐

    

## 拓扑排序

- 编译器通过分析源文件或者程序员事先写好的编译配置文件（比如Makefile文件），来获取这种局部的依赖关系

- 拓扑排序只能在有向无环图上实现，其结果不唯一

- Kahn算法：

  - 定义数据结构的时候，如果s需要先于t执行，那就添加一条s指向t的边。所以，如果某个顶点入度为0， 也就表示，没有任何顶点必须先于这个顶点执行，那么这个顶点就可以执行了。

  - 出一个入度为0的顶点，将其输出到拓扑排序的结果序列中，并且把这个顶点从图中删除（也就是把这个顶点可达的顶点的入度都减1）。我们循环执行上面的过程，直到所有的顶点都被输出。

  - **检测图是否有环**：对于Kahn算法来说，如果最后输出出来的顶点个数，少于图中顶点个数，图中还有入度不是0的顶点，那就说明，图中存在环。

    ```java
    public void topoSortByKahn() {
      int[] inDegree = new int[v]; // 统计每个顶点的入度
      for (int i = 0; i < v; ++i) {
        for (int j = 0; j < adj[i].size(); ++j) {
          int w = adj[i].get(j); // i->w
          inDegree[w]++;
        }
      }
      LinkedList<Integer> queue = new LinkedList<>();
      for (int i = 0; i < v; ++i) {
        if (inDegree[i] == 0) queue.add(i);
      }
      while (!queue.isEmpty()) {
        int i = queue.remove();
        System.out.print("->" + i);
        for (int j = 0; j < adj[i].size(); ++j) {
          int k = adj[i].get(j);
          inDegree[k]--;
          if (inDegree[k] == 0) queue.add(k);
        }
      }
    }
    ```

- DFS算法

  - 深度优先遍历（广度也可以）

    ```java
    public void topoSortByDFS() {
      // 先构建逆邻接表，边s->t表示，s依赖于t，t先于s
      LinkedList<Integer> inverseAdj[] = new LinkedList[v];
      for (int i = 0; i < v; ++i) { // 申请空间
        inverseAdj[i] = new LinkedList<>();
      }
      for (int i = 0; i < v; ++i) { // 通过邻接表生成逆邻接表
        for (int j = 0; j < adj[i].size(); ++j) {
          int w = adj[i].get(j); // i->w
          inverseAdj[w].add(i); // w->i
        }
      }
      boolean[] visited = new boolean[v];
      for (int i = 0; i < v; ++i) { // 深度优先遍历图
        if (visited[i] == false) {
          visited[i] = true;
          dfs(i, inverseAdj, visited);
        }
      }
    }
    
    private void dfs(
        int vertex, LinkedList<Integer> inverseAdj[], boolean[] visited) {
      for (int i = 0; i < inverseAdj[vertex].size(); ++i) {
        int w = inverseAdj[vertex].get(i);
        if (visited[w] == true) continue;
        visited[w] = true;
        dfs(w, inverseAdj, visited);
      } // 先把vertex这个顶点可达的所有顶点都打印出来之后，再打印它自己
      System.out.print("->" + vertex);
    }
    ```

- Kahn算法与DFS算法的都需要遍历每个顶点、每条边，所以时间复杂度是O(V+E)

## 最短路径算法

- 经典的Dijkstra算法，单源
- 地图软件用的更多的是类似A*的启发式搜索算法
- 地图软件要规划的路径太长，如何优化最短路径算法
  - 两点间的最短路径往往集中在一个区块内，可以避免遍历整个大图
  - 比较远的地点，我觉得可以用小方格代替一块地方，从大地图寻找出两个小方格之间的最短路径
